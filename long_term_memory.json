[
  {
    "question": "What is this document ?",
    "answer": "This document appears to be a **summary or record of \"Agentic AI Workshop - Session 1\"**.\n\nIt includes:\n*   An **agenda overview** for the workshop.\n*   **Meeting records/transcript summary** of discussions from that session, specifically regarding the nature of supervisor and sub-agents in an LLM context.",
    "sources": [
      {
        "page_content": "Workshop Agenda Overview Adithya Krishnan outlined the mini workshop's plan, starting with an overview of agents, including what they are and their functions. They mentioned detailed exploration of two agent frameworks: ADK (Agent Development Kit by Google) and LangGraph, with ADK likely to be covered in the first session. The plan also includes an overview of the capstone project involving multi-agents and a final session with a live workshop and Q&A (00:07:13).",
        "metadata": {
          "source": "/tmp/tmp7c7qfmr1.docx"
        }
      },
      {
        "page_content": "Attachments  Agentic AI Workshop -Session 1 \n\nMeeting records Transcript Recording \n\n\n\nSummary",
        "metadata": {
          "source": "/tmp/tmp7c7qfmr1.docx"
        }
      },
      {
        "page_content": "Jun 26, 2025\n\n Agentic AI Workshop -Session 1",
        "metadata": {
          "source": "/tmp/tmp7c7qfmr1.docx"
        }
      },
      {
        "page_content": "Nature of Supervisor and Sub-Agents Sachin Rajendran inquired about the nature of the research team and document authority in a hierarchical setup, asking if they are also LLMs (00:48:13). Adithya Krishnan confirmed that these nodes can be considered LLMs that decide and route between sub-agents and manage the flow of information. Sachin Rajendran also asked about the directionality of information flow in a hierarchical structure (00:49:08). Adithya Krishnan clarified that the supervisor manages the flow, potentially routing outputs from one sub-agent to another before a final response to the user (00:50:05).",
        "metadata": {
          "source": "/tmp/tmp7c7qfmr1.docx"
        }
      }
    ]
  },
  {
    "question": "Summarize this document and create a report like structure ",
    "answer": "**Summary Report: Context Management and LLM Interaction Strategies**\n\n**I. Introduction**\nThis report summarizes key strategies for effective context management and outlines the evolution of interaction methods with Large Language Models (LLMs), as presented by Adithya Krishnan.\n\n**II. Strategies for Context Management**\nAdithya Krishnan detailed several approaches to manage context efficiently, particularly in agentic systems:\n\n*   **Agent State:** Utilizing an agent state to selectively maintain only the most relevant variables across different agents. This helps in keeping the context focused and manageable.\n*   **Summarization vs. Buffer Memory:** For handling longer contexts, a choice exists between summarizing the entire context or employing a buffer memory that retains only the last 'n' outputs. This decision depends on the specific needs for detail retention versus conciseness.\n*   **Vector Store for Context Retrieval:** Storing large volumes of context in a vector store allows agents to retrieve specific, relevant information as and when required. This method is particularly useful for dynamic and extensive knowledge bases.\n\n**III. Evolution of LLM Interaction**\nThe interaction with LLMs has progressed through several stages, each offering enhanced capabilities for accuracy and domain specificity:\n\n*   **Simple Question and Prompt:** The initial and most basic form of interaction, involving direct questions or prompts to the LLM.\n*   **Retrieval-Augmented Generation (RAG):** This advanced technique incorporates more context by using a vector store to retrieve relevant information. RAG is crucial for improving the accuracy of LLM responses, especially for domain-specific, company-specific, or less common data.\n*   **Fine-tuning:** For achieving very specific accuracy requirements, fine-tuning an open-source LLM is an option. This process involves creating and utilizing custom question-answer pairs to train the model on a highly specialized dataset.",
    "sources": [
      {
        "page_content": "Strategies for Context Management Adithya Krishnan presented several strategies for effective context management, including using an agent state to maintain only relevant variables across agents (00:34:20). They also discussed the choice between summarization and buffer memory (considering only the last 'n' outputs) for managing longer contexts (00:35:15). Another method mentioned was storing large amounts of context in a vector store for retrieval as needed by each agent (00:36:08).",
        "metadata": {
          "source": "/tmp/tmpqco8d5o3.docx"
        }
      },
      {
        "page_content": "Attachments  Agentic AI Workshop -Session 1 \n\nMeeting records Transcript Recording \n\n\n\nSummary",
        "metadata": {
          "source": "/tmp/tmpqco8d5o3.docx"
        }
      },
      {
        "page_content": "Raj Suresh Jayaraman Surya Pandurangan sushant kulkarni Sushobhon Karmakar Suthalakkshmi Veluchamy Thirumaalavan Elumalai Udayakumar Devaraj Vamsi Karanam Venkata Rachakonda Venkata Ramireddy Venkateswarlu Vobbani Vignesh Dharmalingam Vishal Surya Vishal Tyagi Yashwant Chouhan Vignesh Mohan Balaji Illuur Shreyan Prashanth Shetty Shyam Ramanan Vinayak Kaithayil",
        "metadata": {
          "source": "/tmp/tmpqco8d5o3.docx"
        }
      },
      {
        "page_content": "Evolution of LLM Interaction Adithya Krishnan illustrated the progression of interacting with LLMs, starting from a simple question and prompt to incorporating more context using techniques like Retrieval-Augmented Generation (RAG) for domain-specific data (00:13:37). They explained that RAG involves using a vector store to retrieve relevant context to improve the accuracy of LLM responses for company-specific or less common information (00:15:01). Adithya Krishnan then discussed fine-tuning as another option for very specific accuracy needs, requiring the creation of question-answer pairs to train an open-source LLM (00:16:03).",
        "metadata": {
          "source": "/tmp/tmpqco8d5o3.docx"
        }
      }
    ]
  },
  {
    "question": "How are you ?",
    "answer": "I don't know.",
    "sources": [
      {
        "page_content": "Jun 26, 2025\n\n Agentic AI Workshop -Session 1",
        "metadata": {
          "source": "/tmp/tmp3x0p_c7q.docx"
        }
      },
      {
        "page_content": "Interactive Session and LLM Context Adithya Krishnan encouraged participants to ask questions at any time to keep the session interactive. They then began by providing a quick overview of the evolution from general-purpose LLMs like GPT-2 and GPT-3 to more recent models like GPT-4 and Gemini 2.0 flash (00:08:33). Adithya Krishnan explained the emergence of reasoning LLMs, which incorporate chain of thought during training to improve step-by-step logic and reduce hallucinations (00:10:02).",
        "metadata": {
          "source": "/tmp/tmp3x0p_c7q.docx"
        }
      },
      {
        "page_content": "Fine-tuning Considerations and Multi-Step Processes Adithya Krishnan pointed out the complexities of fine-tuning, including the need for substantial data and managing the hosting and deployment of the fine-tuned model (00:16:03). They mentioned that cloud platforms offer some simpler fine-tuning methods like SFT. Adithya Krishnan then transitioned to scenarios requiring multi-step processes to answer complex questions, involving dynamic and conditional steps beyond simple summarization or visualization. They explained that such processes might involve code generation, execution, and report creation (00:16:59).",
        "metadata": {
          "source": "/tmp/tmp3x0p_c7q.docx"
        }
      },
      {
        "page_content": "Evolution of LLM Interaction Adithya Krishnan illustrated the progression of interacting with LLMs, starting from a simple question and prompt to incorporating more context using techniques like Retrieval-Augmented Generation (RAG) for domain-specific data (00:13:37). They explained that RAG involves using a vector store to retrieve relevant context to improve the accuracy of LLM responses for company-specific or less common information (00:15:01). Adithya Krishnan then discussed fine-tuning as another option for very specific accuracy needs, requiring the creation of question-answer pairs to train an open-source LLM (00:16:03).",
        "metadata": {
          "source": "/tmp/tmp3x0p_c7q.docx"
        }
      }
    ]
  },
  {
    "question": "What is the buffer memory means ?",
    "answer": "Buffer memory is a limited portion of the short-term memory (context within a workflow) that is used when the full context exceeds its limits.",
    "sources": [
      {
        "page_content": "Buffer Memory vs. Long-Term Memory Vignesh Mohan sought clarification on the difference between buffer memory and long-term memory, asking if buffer memory is a superset of long-term memory. Adithya Krishnan clarified that short-term memory is the context within a workflow, while buffer memory is a limited portion of that context used when it exceeds limits (00:44:36). Long-term memory, on the other hand, involves storing outputs across conversations in databases or vector stores for future reference (00:45:29).",
        "metadata": {
          "source": "/tmp/tmpi6q12alv.docx"
        }
      },
      {
        "page_content": "Reinforcement Learning and Agent Components Anirudha Selvarajan questioned whether reinforcement learning happens across different agent training methods like fine-tuning or RAG-based training. Adithya Krishnan explained that reinforcement learning can be used to fine-tune LLMs based on feedback, but it's generally a separate step from how agents operate (00:23:06). Adithya Krishnan then presented a visual overview of the components within an agent, highlighting the LLM as the core, access to various tools (custom code, vector stores, databases, abstracted tools), and the ability to perform actions like code execution and web search (00:24:45). They also described the concept of short-term (within a workflow) and long-term (across conversations) memory in agents (00:25:51).",
        "metadata": {
          "source": "/tmp/tmpi6q12alv.docx"
        }
      },
      {
        "page_content": "Strategies for Context Management Adithya Krishnan presented several strategies for effective context management, including using an agent state to maintain only relevant variables across agents (00:34:20). They also discussed the choice between summarization and buffer memory (considering only the last 'n' outputs) for managing longer contexts (00:35:15). Another method mentioned was storing large amounts of context in a vector store for retrieval as needed by each agent (00:36:08).",
        "metadata": {
          "source": "/tmp/tmpi6q12alv.docx"
        }
      },
      {
        "page_content": "Output Formatting, Caching, and Code Execution Adithya Krishnan mentioned the capability to define output formats for agents and the potential for caching agent and LLM responses for a certain time period to improve efficiency (00:29:52). They also highlighted the feature of code execution within agent frameworks, allowing agents to run code in sandboxed environments. Streaming of responses for complex tasks was mentioned as another supported capability (00:31:04).",
        "metadata": {
          "source": "/tmp/tmpi6q12alv.docx"
        }
      }
    ]
  },
  {
    "question": "What is long term memory and how to implement that ?",
    "answer": "Long-term memory involves storing outputs across conversations in databases or vector stores for future reference.\n\nTo implement long-term memory, you can:\n*   Store outputs across conversations in databases or vector stores.\n*   Store large amounts of context in a vector store for retrieval as needed by each agent.",
    "sources": [
      {
        "page_content": "Buffer Memory vs. Long-Term Memory Vignesh Mohan sought clarification on the difference between buffer memory and long-term memory, asking if buffer memory is a superset of long-term memory. Adithya Krishnan clarified that short-term memory is the context within a workflow, while buffer memory is a limited portion of that context used when it exceeds limits (00:44:36). Long-term memory, on the other hand, involves storing outputs across conversations in databases or vector stores for future reference (00:45:29).",
        "metadata": {
          "source": "/tmp/tmpzyoxfafw.docx"
        }
      },
      {
        "page_content": "Reinforcement Learning and Agent Components Anirudha Selvarajan questioned whether reinforcement learning happens across different agent training methods like fine-tuning or RAG-based training. Adithya Krishnan explained that reinforcement learning can be used to fine-tune LLMs based on feedback, but it's generally a separate step from how agents operate (00:23:06). Adithya Krishnan then presented a visual overview of the components within an agent, highlighting the LLM as the core, access to various tools (custom code, vector stores, databases, abstracted tools), and the ability to perform actions like code execution and web search (00:24:45). They also described the concept of short-term (within a workflow) and long-term (across conversations) memory in agents (00:25:51).",
        "metadata": {
          "source": "/tmp/tmpzyoxfafw.docx"
        }
      },
      {
        "page_content": "Strategies for Context Management Adithya Krishnan presented several strategies for effective context management, including using an agent state to maintain only relevant variables across agents (00:34:20). They also discussed the choice between summarization and buffer memory (considering only the last 'n' outputs) for managing longer contexts (00:35:15). Another method mentioned was storing large amounts of context in a vector store for retrieval as needed by each agent (00:36:08).",
        "metadata": {
          "source": "/tmp/tmpzyoxfafw.docx"
        }
      },
      {
        "page_content": "Evolution of LLM Interaction Adithya Krishnan illustrated the progression of interacting with LLMs, starting from a simple question and prompt to incorporating more context using techniques like Retrieval-Augmented Generation (RAG) for domain-specific data (00:13:37). They explained that RAG involves using a vector store to retrieve relevant context to improve the accuracy of LLM responses for company-specific or less common information (00:15:01). Adithya Krishnan then discussed fine-tuning as another option for very specific accuracy needs, requiring the creation of question-answer pairs to train an open-source LLM (00:16:03).",
        "metadata": {
          "source": "/tmp/tmpzyoxfafw.docx"
        }
      }
    ]
  },
  {
    "question": "What is the difference bettwen Long and short term memory ?",
    "answer": "Short-term memory is the context that exists **within a workflow**.\n\nLong-term memory, on the other hand, involves storing outputs **across conversations** in databases or vector stores for future reference.",
    "sources": [
      {
        "page_content": "Buffer Memory vs. Long-Term Memory Vignesh Mohan sought clarification on the difference between buffer memory and long-term memory, asking if buffer memory is a superset of long-term memory. Adithya Krishnan clarified that short-term memory is the context within a workflow, while buffer memory is a limited portion of that context used when it exceeds limits (00:44:36). Long-term memory, on the other hand, involves storing outputs across conversations in databases or vector stores for future reference (00:45:29).",
        "metadata": {
          "source": "/tmp/tmpthokoiaz.docx"
        }
      },
      {
        "page_content": "Reinforcement Learning and Agent Components Anirudha Selvarajan questioned whether reinforcement learning happens across different agent training methods like fine-tuning or RAG-based training. Adithya Krishnan explained that reinforcement learning can be used to fine-tune LLMs based on feedback, but it's generally a separate step from how agents operate (00:23:06). Adithya Krishnan then presented a visual overview of the components within an agent, highlighting the LLM as the core, access to various tools (custom code, vector stores, databases, abstracted tools), and the ability to perform actions like code execution and web search (00:24:45). They also described the concept of short-term (within a workflow) and long-term (across conversations) memory in agents (00:25:51).",
        "metadata": {
          "source": "/tmp/tmpthokoiaz.docx"
        }
      },
      {
        "page_content": "Strategies for Context Management Adithya Krishnan presented several strategies for effective context management, including using an agent state to maintain only relevant variables across agents (00:34:20). They also discussed the choice between summarization and buffer memory (considering only the last 'n' outputs) for managing longer contexts (00:35:15). Another method mentioned was storing large amounts of context in a vector store for retrieval as needed by each agent (00:36:08).",
        "metadata": {
          "source": "/tmp/tmpthokoiaz.docx"
        }
      },
      {
        "page_content": "Evolution of LLM Interaction Adithya Krishnan illustrated the progression of interacting with LLMs, starting from a simple question and prompt to incorporating more context using techniques like Retrieval-Augmented Generation (RAG) for domain-specific data (00:13:37). They explained that RAG involves using a vector store to retrieve relevant context to improve the accuracy of LLM responses for company-specific or less common information (00:15:01). Adithya Krishnan then discussed fine-tuning as another option for very specific accuracy needs, requiring the creation of question-answer pairs to train an open-source LLM (00:16:03).",
        "metadata": {
          "source": "/tmp/tmpthokoiaz.docx"
        }
      }
    ]
  },
  {
    "question": "What is this report ?",
    "answer": "This report is a **Baby Medical History Summary**. It details the baby's information (age, weight, feeding) and provides a chronological account of two fever episodes, including symptoms, temperatures, medications administered, and recovery notes.",
    "sources": [
      {
        "page_content": "Baby Medical History Summary\n\nGenerated on: June 19, 2025 at 12:45 PM\n\n\ud83d\udc76 Baby Information\n\nAge: 1.3 years\n\nWeight: 9.3 kg\n\nFeeding: Exclusively breastfed\n\n\ud83d\udcc6 April 13, 2025 \u2013 Fever Episode\n\n\u2022 High fever started during the night (104.7\u00b0F).\n\n\u2022 Given 4 mL of a combination syrup: Ibuprofen (100\u202fmg/5\u202fmL) + Paracetamol (162.5\u202fmg/5\u202fmL).\n\n\u2022 Fever dropped to 99\u00b0F by 3:51 AM.\n\n\u2022 Temperature was 97.5\u00b0F by 9:30 AM. No medication needed.\n\n\u2022 Mild fever returned at 10:45 AM (100.9\u00b0F), another 4 mL dose given.\n\n\u2022 Baby fully recovered the same day.\n\n\ud83d\udcc6 June 2025 \u2013 Fever Episode\n\n\u2022 June 16 (night): Travelled in AC train.\n\n\u2022 June 17 (morning): Returned home.\n\n\u2022 June 18 (morning): Head bath given. Fever appeared by evening.",
        "metadata": {
          "source": "/tmp/tmpqtcib9ig.docx"
        }
      },
      {
        "page_content": "\u2022 June 18 (morning): Head bath given. Fever appeared by evening.\n\n\u2022 June 19, 4:30 AM: Baby woke up and vomited twice. Fever present. Given 5 mL of Paracetamol (250 mg/5 mL).\n\n\u2022 June 19, 11:00 AM: Ate 1 idly, no fever.\n\n\u2022 ~1:00 PM: Fever returned after lunch. Given another 5 mL of Paracetamol.\n\n\u2022 5:00 PM: Fever spiked to 104\u00b0F. Given 5 mL Paracetamol again.",
        "metadata": {
          "source": "/tmp/tmpqtcib9ig.docx"
        }
      }
    ]
  },
  {
    "question": "how many time did he get the fever what are those dates ?",
    "answer": "The baby had a fever on two occasions:\n\n1.  **April 13, 2025**\n2.  **June 18 and 19, 2025**",
    "sources": [
      {
        "page_content": "Baby Medical History Summary\n\nGenerated on: June 19, 2025 at 12:45 PM\n\n\ud83d\udc76 Baby Information\n\nAge: 1.3 years\n\nWeight: 9.3 kg\n\nFeeding: Exclusively breastfed\n\n\ud83d\udcc6 April 13, 2025 \u2013 Fever Episode\n\n\u2022 High fever started during the night (104.7\u00b0F).\n\n\u2022 Given 4 mL of a combination syrup: Ibuprofen (100\u202fmg/5\u202fmL) + Paracetamol (162.5\u202fmg/5\u202fmL).\n\n\u2022 Fever dropped to 99\u00b0F by 3:51 AM.\n\n\u2022 Temperature was 97.5\u00b0F by 9:30 AM. No medication needed.\n\n\u2022 Mild fever returned at 10:45 AM (100.9\u00b0F), another 4 mL dose given.\n\n\u2022 Baby fully recovered the same day.\n\n\ud83d\udcc6 June 2025 \u2013 Fever Episode\n\n\u2022 June 16 (night): Travelled in AC train.\n\n\u2022 June 17 (morning): Returned home.\n\n\u2022 June 18 (morning): Head bath given. Fever appeared by evening.",
        "metadata": {
          "source": "/tmp/tmpywrgw1zw.docx"
        }
      },
      {
        "page_content": "\u2022 June 18 (morning): Head bath given. Fever appeared by evening.\n\n\u2022 June 19, 4:30 AM: Baby woke up and vomited twice. Fever present. Given 5 mL of Paracetamol (250 mg/5 mL).\n\n\u2022 June 19, 11:00 AM: Ate 1 idly, no fever.\n\n\u2022 ~1:00 PM: Fever returned after lunch. Given another 5 mL of Paracetamol.\n\n\u2022 5:00 PM: Fever spiked to 104\u00b0F. Given 5 mL Paracetamol again.",
        "metadata": {
          "source": "/tmp/tmpywrgw1zw.docx"
        }
      }
    ]
  }
]